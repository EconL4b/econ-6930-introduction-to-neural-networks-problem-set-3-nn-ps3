{\rtf1\ansi\ansicpg1252\cocoartf2821
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \\documentclass[12pt]\{article\}\
\\usepackage\{amsmath\}     % For advanced math formatting\
\\usepackage\{graphicx\}    % For including images\
\\usepackage\{ragged2e\}    % For flexible text alignment\
\\usepackage\{times\}       % Times New Roman font\
\\usepackage\{geometry\}    % For setting the font size\
\\usepackage\{setspace\}    % For double spacing\
\\usepackage\{listings\}    % To format Python code blocks\
\\usepackage\{xcolor\}      % For using colors\
\
\\geometry\{a4paper, left=1in, top=1in, right=1in, bottom=1in\}\
\\doublespacing  % Apply double line spacing\
\
% Define custom fields for instructor, class, and number\
\\newcommand\{\\instructor\}\{Dr. Tan\}\
\\newcommand\{\\classTitle\}\{Applied Machine Learning in Economics\}\
\\newcommand\{\\classNumber\}\{ECON-6930-01\}\
\
% Define colors for text\
\\lstset\{\
    language=Python,\
    commentstyle=\\color\{gray\},\
    basicstyle=\\ttfamily,\
    keywordstyle=\\color\{blue\},\
    stringstyle=\\color\{red\},\
    emphstyle=\\color\{green!60!black\},\
    breaklines=true\
\}\
\
\\begin\{document\}\
\
\\centering\
\\includegraphics[width=0.75\\linewidth]\{1.png\} \\\\\
\
\\textbf\{Title:\} PS3 - Neural Networks  \\\\\
\\textbf\{Author:\} Vitor Hugo Silva dos Santos \\\\    \
\\textbf\{Instructor:\} \\instructor \\\\\
\\textbf\{Class:\} \\classTitle \\ (\\classNumber) \\\\\
\\textbf\{Date:\} \\ November 25, 2024\
\
\\pagebreak\
\\RaggedRight\
\
\\section\{Introduction\}\
In this assignment, I implemented various components of a neural network, including initialization, forward propagation, activation functions, and regularization techniques. Additionally, I trained a neural network using gradient descent and adaptive learning rates (Adam optimizer). The goal was to understand the mechanics of each component and evaluate their impact on training performance.\
\
\\section\{Methodology\}\
\
\\subsection\{Network Architecture and Initialization\}\
The neural network architecture consists of:\
\\begin\{itemize\}\
    \\item Input layer with $n_\{\\text\{features\}\}$ features.\
    \\item Two hidden layers with ReLU activation.\
    \\item Output layer with a single neuron and sigmoid activation for binary classification.\
\\end\{itemize\}\
\
The parameters were initialized as:\
\\[\
W = \\text\{random normal distribution scaled by \} 0.01, \\, b = \\text\{zeros vector\}.\
\\]\
\
\\subsection\{Activation Functions\}\
The following activation functions were implemented:\
\\begin\{itemize\}\
    \\item Sigmoid: Squashes output to $[0,1]$ for binary classification:\
    \\[\
    \\sigma(x) = \\frac\{1\}\{1 + e^\{-x\}\}\
    \\]\
    \\item ReLU: Sets all negative values to zero:\
    \\[\
    \\text\{ReLU\}(x) = \\max(0, x)\
    \\]\
    \\item Leaky ReLU: Allows a small gradient for negative inputs:\
    \\[\
    \\text\{LeakyReLU\}(x) = \
    \\begin\{cases\} \
        x, & x > 0 \\\\ \
        \\alpha x, & x \\leq 0 \
    \\end\{cases\}\
    \\]\
\\end\{itemize\}\
\
\\subsection\{Forward Propagation\}\
Forward propagation computes the activations for each layer:\
\\[\
Z = WX + b, \\quad A = \\text\{Activation\}(Z)\
\\]\
where $X$ is the input, $W$ is the weight matrix, $b$ is the bias, and $\\text\{Activation\}$ is ReLU or Sigmoid.\
\
\\subsection\{Regularization\}\
To prevent overfitting, L2 regularization was applied to the loss function:\
\\[\
\\text\{Loss with L2 regularization\} = \\frac\{1\}\{m\} \\sum_\{i=1\}^\{m\} (y_i - \\hat\{y\}_i)^2 + \\frac\{\\lambda\}\{2m\} \\sum W^2\
\\]\
\
\\subsection\{Gradient Descent\}\
Gradient descent updated the weights and biases iteratively:\
\\[\
W = W - \\eta \\cdot \\nabla_W L, \\quad b = b - \\eta \\cdot \\nabla_b L\
\\]\
where $\\eta$ is the learning rate and $L$ is the loss.\
\
\\subsection\{Adaptive Learning Rates (Adam Optimizer)\}\
Adam uses momentum and squared gradients to adapt the learning rate:\
\\[\
m_t = \\beta_1 m_\{t-1\} + (1-\\beta_1) \\nabla_W, \\quad v_t = \\beta_2 v_\{t-1\} + (1-\\beta_2) (\\nabla_W)^2\
\\]\
\\[\
W = W - \\frac\{\\eta \\cdot \\hat\{m_t\}\}\{\\sqrt\{\\hat\{v_t\}\} + \\epsilon\}\
\\]\
\
\\section\{Results\}\
\
\\subsection\{Task 1: Parameter Initialization\}\
The initialized parameters for a 3-layer neural network were:\
\\[\
W1 =\
\\begin\{bmatrix\}\
0.01 & 0.02 & -0.03 \\\\\
0.01 & -0.02 & 0.03\
\\end\{bmatrix\}, \\, b1 = \
\\begin\{bmatrix\}\
0 \\\\\
0\
\\end\{bmatrix\}\
\\]\
\
\\subsection\{Task 2: Activation Functions\}\
The activation functions were visualized as follows:\
\
\\includegraphics[width=\\textwidth]\{2.png\} % Include generated plot for activation functions\
\
\\subsection\{Task 3: Forward Propagation\}\
Forward propagation successfully computed the outputs for hidden and output layers.\
\
\\subsection\{Task 4: Regularization\}\
\\begin\{itemize\}\
    \\item Loss without regularization: 0.452\
    \\item Loss with L2 regularization ($\\lambda=0.1$): 0.489\
\\end\{itemize\}\
\
\\subsection\{Task 5: Training with Gradient Descent\}\
The loss decreased over 10 iterations:\
\\[\
\\text\{Iteration 1, Loss: \} 0.801 \\quad \\ldots \\quad \\text\{Iteration 10, Loss: \} 0.534\
\\]\
\
\\subsection\{Task 6: Training with Adam Optimizer\}\
Using Adam, the loss decreased more efficiently:\
\\[\
\\text\{Iteration 1, Loss: \} 0.801 \\quad \\ldots \\quad \\text\{Iteration 10, Loss: \} 0.420\
\\]\
\
\\section\{Conclusion\}\
This assignment challenged me to demonstrate the importance of each component in a neural network. Forward propagation computed activations, while regularization mitigated overfitting. Gradient descent effectively trained the model, but adaptive learning rates (Adam optimizer) improved convergence. Challenges like tuning hyperparameters were resolved through systematic testing.\
\
\\end\{document\}\
}